{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from urllib.request import urlopen\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import duckdb as ddb\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from itertools import combinations    \n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql duckdb:///eda-ddb/eda-gdelt.ddb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql duckdb:///eda-ddb/eda-yfinance.ddb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Basic eda </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global ddb as a parquet by using the cli\n",
    "df2 = dd.read_parquet('output2.parquet').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Date'] = pd.to_datetime(df2['Day'], format='%Y%m%d')\n",
    "df2 = df2[df2['Date'].dt.year >= 2018]\n",
    "df2 = df2.drop('Day', axis=1)\n",
    "mean_df = df2.groupby('Date').mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_df['GoldsteinScale'])\n",
    "plt.title('Goldstein Scale average over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mean_df['NumMentions'], label='NumMentions_avg')\n",
    "plt.title('NumMentions average over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df2['GoldsteinScale'] )\n",
    "plt.title('Goldstein Scale Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df2['NumMentions'],df2['GoldsteinScale'], 'o')\n",
    "plt.title('NumMentions vs Goldstein Scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag anaylysis for the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select *\n",
    "from INFORMATION_SCHEMA.COLUMNS\n",
    "where TABLE_NAME='yfinance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = %sql SELECT * FROM yfinance\n",
    "stock_df\n",
    "stock_df = stock_df.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import pacf, acf\n",
    "\n",
    "\n",
    "def make_pacf_plot(df, stockname, lag):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_pacf(df['Close'], lags=lag)\n",
    "    plt.title(f'Partial Autocorrelation Function (PACF) of {stockname} Stock Close Prices')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Partial Autocorrelation')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def make_acf_plot(df,stockname, lag):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(df['Close'], lags=lag)\n",
    "    plt.title(f'Autocorrelation Function (ACF) of {stockname} Stock Close Prices')\n",
    "    plt.xlabel('Lags')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def make_lag_plot(df, stockname, t):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    lag_plot(df['Close'], lag=t)\n",
    "    plt.title(f'Lag Plot of {stockname} Stock Close Prices')\n",
    "    plt.xlabel('Close Price (t)')\n",
    "    plt.ylabel(f'Close Price (t + {t})')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def get_all_stock_names(df):\n",
    "    return df['Stock'].unique()\n",
    "\n",
    "\n",
    "\n",
    "def first_non_significant_acf_lag(series, alpha=0.05, nlags=100):\n",
    "    acf_values, confint = acf(series, alpha=alpha, nlags=nlags)\n",
    "    confint = confint - acf_values[:, None]\n",
    "    for lag in range(1, len(acf_values)):\n",
    "        if not (acf_values[lag] < confint[lag, 0] or acf_values[lag] > confint[lag, 1]):\n",
    "            return(lag)\n",
    "    return None\n",
    "\n",
    "def first_non_significant_pacf_lag(series, alpha=0.05, nlags=100):\n",
    "    acf_values, confint = pacf(series, alpha=alpha, nlags=nlags)\n",
    "    confint = confint - acf_values[:, None]\n",
    "    for lag in range(1, len(acf_values)):\n",
    "        if not (acf_values[lag] < confint[lag, 0] or acf_values[lag] > confint[lag, 1]):\n",
    "            return(lag)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def avg_non_sig_acf_each_sector():\n",
    "    all_sectors = stock_df['Sector'].unique()\n",
    "    for sector in all_sectors:\n",
    "        sector_df = stock_df[stock_df['Sector'] == sector]\n",
    "\n",
    "        unique_comp = sector_df['Stock'].unique()\n",
    "        for comp in unique_comp:\n",
    "            comp_df = sector_df[sector_df['Stock'] == comp]\n",
    "            avg_lag = first_non_significant_acf_lag(comp_df['Close'])\n",
    "\n",
    "        print(f'Sector: {sector}, Average Non-Significant ACF Lag: {avg_lag}')\n",
    "\n",
    "def avg_non_sig_acf_company_in_sector(sector):\n",
    "    sector_df = stock_df[stock_df['Sector'] == sector]\n",
    "    unique_comp = sector_df['Stock'].unique()\n",
    "    for comp in unique_comp:\n",
    "        comp_df = sector_df[sector_df['Stock'] == comp]\n",
    "        avg_lag = first_non_significant_acf_lag(comp_df['Close'])\n",
    "        print(f'Sector: {sector}, Company: {comp}, Average Non-Significant ACF Lag: {avg_lag}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sector in stock_df['Sector'].unique():\n",
    "    for company in stock_df[stock_df['Sector'] == sector]['Stock'].unique():\n",
    "        companyseries = stock_df[(stock_df['Sector'] == sector) & (stock_df['Stock'] == company)]['Close']\n",
    "        print(f'Sector: {sector}, Company: {company}, First Non-Significant PACF Lag: {first_non_significant_pacf_lag(companyseries)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock decomposition \n",
    "\n",
    "def stock_decomposition_plot(stock_name, month_start, month_end, year_start, year_end, period):\n",
    "    if stock_name =='AAPL':\n",
    "        stock_plot = stock_df[stock_df['Stock'] == stock_name]\n",
    "        stock_plot.set_index('Date', inplace=True)\n",
    "        stock_plot['Day'] = stock_plot.index.day\n",
    "        stock_plot['Month'] = stock_plot.index.month\n",
    "        stock_plot['Year'] = stock_plot.index.year\n",
    "        stock_plot = stock_plot.sort_values(by='Date')\n",
    "    else:\n",
    "        stock_plot = stock_df[stock_df['Stock'] == stock_name]\n",
    "        stock_plot.set_index('Date', inplace=True)\n",
    "        stock_plot['Day'] = stock_plot.index.day\n",
    "        stock_plot['Month'] = stock_plot.index.month\n",
    "        stock_plot['Year'] = stock_plot.index.year\n",
    "    stock_plot = stock_plot[(stock_plot['Month'] >= month_start) & (stock_plot['Month'] <= month_end) & (stock_plot['Year'] >= year_start) & (stock_plot['Year'] <= year_end)]\n",
    "    df_plot = stock_plot\n",
    "    result = seasonal_decompose(df_plot['Close'], model='additive', period=period)\n",
    "    result.plot()\n",
    "    plt.gcf().set_size_inches(12, 8)\n",
    "    plt.suptitle(f'Decomposition of {stock_name} Stock Close Prices', fontsize=10, y = 0)\n",
    "    plt.tick_params(axis='x', rotation = 45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for stock_name in stock_df['Stock'].unique():\n",
    "    stock_decomposition_plot(stock_name, 1, 12, 2019, 2023, 365)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_in_each_sector():\n",
    "    all_sectors = stock_df['Sector'].unique()\n",
    "    for sector in all_sectors:\n",
    "        print(f'Sector: {sector}')\n",
    "        all_comp = stock_df[stock_df['Sector'] == sector]['Stock'].unique()\n",
    "        all_combinations = (combinations(all_comp, 2))\n",
    "        filtered_combinations = list(filter(lambda x: x[0] != x[1], all_combinations))\n",
    "        for comp1, comp2 in filtered_combinations:\n",
    "            comp1_df = stock_df[stock_df['Stock'] == comp1]\n",
    "            comp2_df = stock_df[stock_df['Stock'] == comp2]\n",
    "            comp1_df = comp1_df.set_index('Date')\n",
    "            comp2_df = comp2_df.set_index('Date')\n",
    "            comp1_series= comp1_df['Close']\n",
    "            comp2_series = comp2_df['Close'] \n",
    "            concatenated_df = pd.merge(comp1_series, comp2_series, on='Date')\n",
    "            concatenated_df = concatenated_df.dropna()\n",
    "            ret = pearsonr(concatenated_df['Close_x'], concatenated_df['Close_y'])\n",
    "            print(f'Company: {comp1}, Company:{comp2}, Correlation: {ret[0]}, P-Value: {ret[1]}')\n",
    "\n",
    "        # print(filtered_combinations)\n",
    "\n",
    "correlation_in_each_sector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ai fitler </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# NOTE: ollama must be running for this to work, start the ollama app or run `ollama serve`\n",
    "model = 'llama3.2' # TODO: update this for whatever model you wish to use\n",
    "\n",
    "\n",
    "def generate(prompt, context):\n",
    "    r = requests.post('http://localhost:11434/api/generate',\n",
    "                      json={\n",
    "                          'model': model,\n",
    "                          'prompt': prompt,\n",
    "                          'context': context,\n",
    "                      },\n",
    "                      stream=True)\n",
    "    r.raise_for_status()\n",
    "    response =\"\"\n",
    "    for line in r.iter_lines():\n",
    "        body = json.loads(line)\n",
    "        response_part = body.get('response', '')\n",
    "        # # the response streams one token at a time, print that as we receive it\n",
    "        # print(response_part, end='', flush=True)\n",
    "        # save response to a string that returns when the response is done\n",
    "        response += response_part\n",
    "        \n",
    "\n",
    "        if 'error' in body:\n",
    "            raise Exception(body['error'])\n",
    "\n",
    "        if body.get('done', False):\n",
    "            return response\n",
    "\n",
    "# This gonna take 15000 days to run\n",
    "def ai_filter(url, sector):\n",
    "    prompt = f\"Given the URL {url} and sector name {sector}, please follow these steps to assess wheter the headlines on the webpage have a measureable effect on the stock price of the specified sector.\"\n",
    "    prompt += \"1.Headline Extraction: Extract and summarize the key informatoin from the URL.\"\n",
    "    prompt += f\"2.Relevance to sector Performance:Based on the content of the URL, determine if they contain information that could influence stock price of {sector}., For example, look for news related to company performance, earnings reports, product launches, regulatory decisions, market trends, or significant events (mergers, acquisitions, etc.). \"\n",
    "    prompt += \"3.Conclusion: State whether or not the content are likely to have a substantial direct impact on the stock. give rational connection between the headlines and stock performance. Provide specific reasons to support your conclusion\"\n",
    "    prompt += \"4. Finally: from the previous conclusion, answer this question in one word: 'yes' or 'no' without punctuation in a newline: 'Do the headlines have a measureable effect on the stock price of the specified company?'\"\n",
    "\n",
    "    context = []\n",
    "    response = generate(prompt, context)\n",
    "    tokens = word_tokenize(response)\n",
    "    # print(response)\n",
    "    # Check if \"yes\" or \"no\" is in the response\n",
    "    if tokens[-1].lower()  == \"yes\":\n",
    "        return True\n",
    "    elif tokens[-1].lower() == \"no\":\n",
    "        return False\n",
    "    else:\n",
    "        # i'll be greedy and assume false\n",
    "        return False\n",
    "   \n",
    "ai_filter(\"indiantimes.com/bakivsjackhanmaTMR\", \"Tech\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data_ai_test= %sql SELECT * FROM gdelt WHERE Day BETWEEN 20241002 AND 20241003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data_ai_test = global_data_ai_test.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out some data to test and speed up the ai \n",
    "mean_std1 = global_data_ai_test['NumMentions'].std() + global_data_ai_test['NumMentions'].mean()\n",
    "filtered_event = global_data_ai_test[global_data_ai_test['NumMentions'] > mean_std1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This took two days in real life which is too long\n",
    "fitlered2_ret = filtered_event[filtered_event['SOURCEURL'].apply(lambda x: ai_filter(x, sector='tech'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitlered2_ret.to_parquet('fitlered2_ret.parquet', index=False)\n",
    "# Save the data to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = dd.read_parquet('fitlered2_ret.parquet').compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questino 5\n",
    "import scipy.stats  as stats\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "appl_news = pd.read_csv(\"stock_related_articles/stocks_to_keywords_broad/AAPL_apple.csv\")\n",
    "appl_news['Date'] = pd.to_datetime(appl_news['Date'])\n",
    "appl_news = appl_news[(appl_news['Date'].dt.year >= 2018) & (appl_news['Date'].dt.year <= 2024)]\n",
    "\n",
    "aapl_stock = stock_df[stock_df['Stock'] == 'AAPL']\n",
    "\n",
    "aapl_stock = aapl_stock[(aapl_stock['Date'].dt.year >= 2018) & (aapl_stock['Date'].dt.year <= 2025)]\n",
    "\n",
    "# print(nvidia_stock.head())\n",
    "\n",
    "def flat_find_mean_correlation(df1, df2, lag):\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').mean()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].shift(-lag)\n",
    "    df1 = df1['Close']\n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "    # print(concatenated_df.head())\n",
    "    # print(concatenated_df.head())\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldsteinScale'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "\n",
    "def flat_find_weightedmean_correlation(df1, df2, lag):\n",
    "\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').mean()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].shift(-lag)\n",
    "    df1 = df1['Close']\n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    concatenated_df['GoldenWeighted'] = concatenated_df['GoldsteinScale'] * concatenated_df['NumMentions']\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldenWeighted'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "\n",
    "def flat_find_sqrt_correl(df1, df2, lag, year):\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').mean()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].shift(-lag)\n",
    "    df1 = df1['Close']\n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    concatenated_df['GoldenWeighted'] = concatenated_df['GoldsteinScale'] * np.sqrt(concatenated_df['NumMentions'])\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldenWeighted'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "    \n",
    "def percentage_find_mean_correlation(df1, df2):\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').mean()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].pct_change() \n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    concatenated_df['GoldenWeighted'] = concatenated_df['GoldsteinScale'] * (concatenated_df['NumMentions'])\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldenWeighted'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "\n",
    "\n",
    "def percentage_find_sqrt_mean_correlation(df1, df2):\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').mean()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].pct_change() \n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    concatenated_df['GoldenWeighted'] = concatenated_df['GoldsteinScale'] * np.sqrt(concatenated_df['NumMentions'])\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldenWeighted'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "\n",
    "def percentage_find_log_mean_correlation(df1, df2):\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').mean()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].pct_change() \n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    concatenated_df['GoldenWeighted'] = concatenated_df['GoldsteinScale'] * np.log(concatenated_df['NumMentions'])\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldenWeighted'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "\n",
    "def percentage_find_sum_correlation(df1, df2):\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').sum()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].pct_change() \n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    concatenated_df['GoldenWeighted'] = concatenated_df['GoldsteinScale'] \n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldenWeighted'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "def percentage_find_sum_log_correlation(df1, df2):\n",
    "    df2 = df2[['Date', 'GoldsteinScale', 'NumMentions']]\n",
    "    df2 = df2.groupby('Date').sum()\n",
    "    df1 = df1.set_index('Date')\n",
    "    df1['Close'] = df1['Close'].pct_change() \n",
    "    concatenated_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    concatenated_df['GoldenWeighted'] = concatenated_df['GoldsteinScale'] * np.log(concatenated_df['NumMentions'])\n",
    "    concatenated_df = concatenated_df.dropna()\n",
    "    ret = stats.pearsonr(concatenated_df['Close'], concatenated_df['GoldenWeighted'])\n",
    "    # return corrlatoin and p_val\n",
    "    return ret[0], ret[1]\n",
    "\n",
    "\n",
    "# percentage_find_mean_correlation(aapl_df, aapl_stock, 1, 2024)\n",
    "\n",
    "# print(percentage_find_sqrt_mean_correlation(nvidia_stock, gd, 1, 2024))\n",
    "# print(flat_find_mean_correlation(aapl_stock, gd,1, 2024))\n",
    "# print(percentage_find_sqrt_mean_correlation(aapl_stock, gd, 2024) ) \n",
    "# print(percentage_find_mean_correlation(aapl_stock, gd, 2024))\n",
    "\n",
    "# print(flat_find_mean_correlation(aapl_stock, appl_news, 1))\n",
    "# print(flat_find_weightedmean_correlation(aapl_stock, appl_news, 1))\n",
    "# print(flat_find_sqrt_correl(aapl_stock, appl_news, 1, 2024))\n",
    "# print(percentage_find_mean_correlation(aapl_stock, appl_news))\n",
    "# print(percentage_find_sqrt_mean_correlation(aapl_stock, appl_news))\n",
    "# print(percentage_find_log_mean_correlation(aapl_stock, appl_news))\n",
    "print(percentage_find_sum_correlation(aapl_stock, appl_news))\n",
    "print(percentage_find_sum_log_correlation(aapl_stock, appl_news))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
