{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install twitter_scraper_selenium --quiet\n",
    "! pip install selenium pandas webdriver-manager --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_scraper_selenium import scrape_keyword\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_twitter(driver, username, password):\n",
    "    driver.get(\"https://twitter.com/login\")\n",
    "    time.sleep(2)  # Wait for the login page to load\n",
    "    \n",
    "    username_input = driver.find_element(By.NAME, \"text\")\n",
    "    username_input.send_keys(username)\n",
    "    username_input.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)  # Wait for the next page to load\n",
    "    \n",
    "    password_input = driver.find_element(By.NAME, \"password\")\n",
    "    password_input.send_keys(password)\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "    time.sleep(5)  # Wait for login to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_keyword(headless, keyword, browser, tweets_count, filename, output_format, since):\n",
    "    tweets = []\n",
    "    browser.get(f\"https://twitter.com/search?q={keyword}%20since%3A{since}&src=typed_query\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scroll multiple times to load more tweets\n",
    "    for _ in range(15):\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    # Get tweet elements\n",
    "    tweet_elements = browser.find_elements(By.XPATH, '//article')\n",
    "\n",
    "    print(f\"Total tweet elements found: {len(tweet_elements)}\")  # Debugging output\n",
    "\n",
    "    for tweet in tweet_elements[:tweets_count]:\n",
    "        try:\n",
    "            # Skip if it's a reply (check for 'Replying to' indicator)\n",
    "            is_reply = tweet.find_elements(By.XPATH, './/div[contains(text(), \"Replying to\")]')\n",
    "            if is_reply:\n",
    "                print(\"Skipping a reply tweet.\")  # Debugging\n",
    "                continue  # Skip this tweet if it's a reply\n",
    "            \n",
    "            username = tweet.find_element(By.XPATH, './/span[contains(text(), \"@\")]').text\n",
    "            tweet_text = tweet.find_element(By.XPATH, './/div[@data-testid=\"tweetText\"]').text\n",
    "            \n",
    "            try:\n",
    "                like_button = tweet.find_element(By.XPATH, './/button[@data-testid=\"like\"]')\n",
    "                likes = like_button.get_attribute(\"aria-label\").split()[0]  # Extract number from label\n",
    "            except NoSuchElementException:\n",
    "                likes = \"0\"\n",
    "\n",
    "            try:\n",
    "                retweet_button = tweet.find_element(By.XPATH, './/button[@data-testid=\"retweet\"]')\n",
    "                retweets = retweet_button.get_attribute(\"aria-label\").split()[0]  # Extract number from label\n",
    "            except NoSuchElementException:\n",
    "                retweets = \"0\"\n",
    "\n",
    "            try:\n",
    "                reply_button = tweet.find_element(By.XPATH, './/button[@data-testid=\"reply\"]')\n",
    "                replies = reply_button.get_attribute(\"aria-label\").split()[0]  # Extract number from label\n",
    "            except NoSuchElementException:\n",
    "                replies = \"0\"\n",
    "\n",
    "            try:\n",
    "                views_span = tweet.find_element(By.XPATH, './/span[contains(text(), \"Views\")]')\n",
    "                views = views_span.text.split()[0]  # Extract the number from text\n",
    "            except NoSuchElementException:\n",
    "                views = \"0\"\n",
    "\n",
    "            date = tweet.find_element(By.XPATH, './/time').get_attribute('datetime')\n",
    "            hashtags = [hashtag.text for hashtag in tweet.find_elements(By.XPATH, './/a[contains(@href,\"/hashtag/\")]')]\n",
    "            media_urls = [img.get_attribute('src') for img in tweet.find_elements(By.XPATH, './/img')]\n",
    "            mentions = [mention.text for mention in tweet.find_elements(By.XPATH, './/a[contains(@href, \"/\")]')]\n",
    "            language = tweet.get_attribute('lang')\n",
    "\n",
    "            tweets.append({\n",
    "                'username': username,\n",
    "                'tweet': tweet_text,\n",
    "                'replies': replies,\n",
    "                'likes': likes,\n",
    "                'retweets': retweets,\n",
    "                'views': views,\n",
    "                'date': date,\n",
    "                'hashtags': hashtags,\n",
    "                'media_urls': media_urls,\n",
    "                'mentions': mentions,\n",
    "                'language': language,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting tweet data: {e}\")\n",
    "\n",
    "    print(f\"Number of tweets scraped: {len(tweets)}\")\n",
    "\n",
    "    # Save to CSV if there are tweets\n",
    "    if output_format == \"csv\" and tweets:\n",
    "        pd.DataFrame(tweets).to_csv(filename + '.csv', index=False)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_profile_tweets(username: str, twitter_username: str, twitter_password: str):\n",
    "    kword = \"from:\" + username\n",
    "    path = './users/' + username\n",
    "    file_path = path + '.csv'\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    \n",
    "    login_twitter(driver, twitter_username, twitter_password)\n",
    "\n",
    "    # Scrape tweets\n",
    "    scrape_keyword(\n",
    "        headless=False,         # Runs scraping in headless mode (no GUI)\n",
    "        keyword=kword,          # Search keyword, e.g., 'from:elonmusk'\n",
    "        browser=driver,         # Uses logged-in Chrome browser for scraping\n",
    "        tweets_count=10000,     # Number of tweets to scrape\n",
    "        filename=path,          # File path to save the output\n",
    "        output_format=\"csv\",    # Saves data in CSV format\n",
    "        since=\"2020-01-01\",     # Retrieves tweets since 2023\n",
    "    )\n",
    "\n",
    "    # Read the saved CSV and return data\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = json.loads(data.to_json(orient='records'))\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"No data found in the CSV file.\")\n",
    "        data = []\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweet elements found: 29\n",
      "Number of tweets scraped: 29\n"
     ]
    }
   ],
   "source": [
    "tweets_data = scrape_profile_tweets('elonmusk', 'toutouxihuannii', 'lingraisin31839')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
